{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924e989c-eeec-48bf-9e23-c92f1a897627",
   "metadata": {},
   "source": [
    "# ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a36c8-1241-4f96-b3df-862f24c3aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image, ImageDraw\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.image_transforms import center_to_corners_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f696cd-a6a4-408f-9e63-3f7a43697163",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ae754-aedb-4081-b67e-8c919825ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb settings\n",
    "wandb_project_name = \"BCCD\"\n",
    "artifact_name = \"\"\n",
    "\n",
    "# local settings\n",
    "use_local_model = False\n",
    "dataset_path = \"BCCD_dataset\"\n",
    "output_dir = \"BCCD_output\"\n",
    "\n",
    "# modeling settings\n",
    "num_train_epochs = 1\n",
    "per_device_train_batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea7e02-2c7f-4fff-a86e-aa1877e84f46",
   "metadata": {},
   "source": [
    "## データ & モデルロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b51037-61c4-45ca-820b-11b61891b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(dataset_path)\n",
    "id2label = {0: \"RBC\", 1: \"WBC\", 2: \"Platelets\"}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca84cc3-c0f1-4d38-807c-cf08a233663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = wandb_project_name\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "run = wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05b0eb-a764-41cb-ac7d-bf202fb32da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if artifact_name != \"\":\n",
    "    artifact = run.use_artifact(artifact_name, type=\"model\")\n",
    "    checkpoint = artifact.download()\n",
    "elif use_local_model:\n",
    "    checkpoint = output_dir + \"/model\"\n",
    "else:\n",
    "    checkpoint = \"facebook/detr-resnet-50\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03465f68-a83f-429b-9b6d-aef5ead42d97",
   "metadata": {},
   "source": [
    "## 前処理準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ad9de-4dc2-42a2-b8b8-76a58adee50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image_annotations_as_coco(image_id, categories, areas, bboxes):\n",
    "    annotations = []\n",
    "    for category, area, bbox in zip(categories, areas, bboxes):\n",
    "        formatted_annotation = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category,\n",
    "            \"iscrowd\": 0,\n",
    "            \"area\": area,\n",
    "            \"bbox\": list(bbox),\n",
    "        }\n",
    "        annotations.append(formatted_annotation)\n",
    "\n",
    "    return {\n",
    "        \"image_id\": image_id,\n",
    "        \"annotations\": annotations,\n",
    "    }\n",
    "\n",
    "\n",
    "train_augment_and_transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Perspective(p=0.1),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.RandomBrightnessContrast(p=0.5),\n",
    "        albumentations.HueSaturationValue(p=0.1),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(\n",
    "        format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "validation_transform = albumentations.Compose(\n",
    "    [albumentations.NoOp()],\n",
    "    bbox_params=albumentations.BboxParams(\n",
    "        format=\"coco\", label_fields=[\"category\"], clip=True\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def augment_and_transform_batch(\n",
    "    examples, transform, image_processor, return_pixel_mask=False\n",
    "):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    for image_id, image, objects in zip(\n",
    "        examples[\"image_id\"], examples[\"image\"], examples[\"objects\"]\n",
    "    ):\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        output = transform(\n",
    "            image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"]\n",
    "        )\n",
    "        images.append(output[\"image\"])\n",
    "\n",
    "        formatted_annotations = format_image_annotations_as_coco(\n",
    "            image_id, output[\"category\"], objects[\"area\"], output[\"bboxes\"]\n",
    "        )\n",
    "        annotations.append(formatted_annotations)\n",
    "\n",
    "    result = image_processor(\n",
    "        images=images, annotations=annotations, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    if not return_pixel_mask:\n",
    "        result.pop(\"pixel_mask\", None)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "train_transform_batch = partial(\n",
    "    augment_and_transform_batch,\n",
    "    transform=train_augment_and_transform,\n",
    "    image_processor=image_processor,\n",
    ")\n",
    "\n",
    "\n",
    "validation_transform_batch = partial(\n",
    "    augment_and_transform_batch,\n",
    "    transform=validation_transform,\n",
    "    image_processor=image_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0dd740-e85e-40a9-9aa5-7688ef8554ed",
   "metadata": {},
   "source": [
    "## 前処理実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf805b21-413f-4607-9090-9df889835647",
   "metadata": {},
   "source": [
    "### tips\n",
    "trainer.evaluate であったり trainer.train の 評価時に， batch の中のデータ数が 1 つしかないと評価予測の返り値の形式が変わってしまうため，カスタム評価関数がうまく動かない場合があるため注意\n",
    "\n",
    "→ 今回は TrainingArguments の引数 per_device_eval_batch_size のデフォルト値 8 で割った余りが 1 にならないように dataset 準備時に test_size=100 として対策している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8378baf-f4c2-4cfb-8bc9-73a736a39740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # サンプリング\n",
    "# dataset = dataset.shuffle(seed=42).select(range(int(0.1 * len(dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e2cfd-a504-49a4-b00e-2ae0a6c4b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=100, seed=0)\n",
    "dataset[\"train\"] = dataset[\"train\"].with_transform(train_transform_batch)\n",
    "dataset[\"test\"] = dataset[\"test\"].with_transform(validation_transform_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a7530-4b7d-4d3a-bd24-1624b878cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdc79d-cc7d-4eb1-8878-3db50f3b812e",
   "metadata": {},
   "source": [
    "## ファインチューニング準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e6f2d-1ae6-44fb-9d80-7d0349c7cb15",
   "metadata": {},
   "source": [
    "### Preparing function to compute mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e5222-ad82-4d2d-85c9-8fedaf9ef5c7",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/main/en/tasks/object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd94a20-e0d6-402e-a0d4-1d034807b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "    height, width = image_size\n",
    "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "    return boxes\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n",
    "    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "    image_sizes = []\n",
    "    post_processed_targets = []\n",
    "    post_processed_predictions = []\n",
    "\n",
    "    for batch in targets:\n",
    "        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n",
    "        image_sizes.append(batch_image_sizes)\n",
    "        for image_target in batch:\n",
    "            boxes = torch.tensor(image_target[\"boxes\"])\n",
    "            boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n",
    "            labels = torch.tensor(image_target[\"class_labels\"])\n",
    "            post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "    for batch, target_sizes in zip(predictions, image_sizes):\n",
    "        batch_logits, batch_boxes = batch[1], batch[2]\n",
    "        output = ModelOutput(\n",
    "            logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes)\n",
    "        )\n",
    "        post_processed_output = image_processor.post_process_object_detection(\n",
    "            output, threshold=threshold, target_sizes=target_sizes\n",
    "        )\n",
    "        post_processed_predictions.extend(post_processed_output)\n",
    "\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "    metric.update(post_processed_predictions, post_processed_targets)\n",
    "    metrics = metric.compute()\n",
    "\n",
    "    classes = metrics.pop(\"classes\")\n",
    "    map_per_class = metrics.pop(\"map_per_class\")\n",
    "    mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "    for class_id, class_map, class_mar in zip(\n",
    "        classes, map_per_class, mar_100_per_class\n",
    "    ):\n",
    "        class_name = (\n",
    "            id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "        )\n",
    "        metrics[f\"map_{class_name}\"] = class_map\n",
    "        metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "    metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "eval_compute_metrics_fn = partial(\n",
    "    compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ad9f6-5512-4790-bcde-64c6e7e74647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f46cb7-37f1-48f5-a5bd-15ca10e59293",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir + \"/log\",\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=1e-4,\n",
    "    max_grad_norm=0.01,\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4e3ba-fcde-4931-9664-c2a0bf4c1ba7",
   "metadata": {},
   "source": [
    "## ファインチューニング実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217c356-bfb8-4910-8047-b4f37fce8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705f181-9e7b-466d-9743-0c99f8df1e3e",
   "metadata": {},
   "source": [
    "## モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9626ff-3cd2-4681-aba5-6a6e70a872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir + \"/model\")\n",
    "image_processor.save_pretrained(output_dir + \"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb39424-2f54-4e46-86d8-da2906a56792",
   "metadata": {},
   "source": [
    "## W&B Table 作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da99df-3563-4ead-8170-b38dc5c57049",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"BCCD/JPEGImages/\"\n",
    "test_batch_size = 8\n",
    "\n",
    "batch_sizes = [\n",
    "    test_batch_size for x in range(int(dataset[\"test\"].num_rows / test_batch_size))\n",
    "]\n",
    "left_batch_size = np.mod(dataset[\"test\"].num_rows, test_batch_size)\n",
    "if left_batch_size != 0:\n",
    "    batch_sizes.append(left_batch_size)\n",
    "\n",
    "image_filenames = os.listdir(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cc33b-cb4c-43e9-8c3d-335a4a476adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table\n",
    "table = wandb.Table(columns=[\"image_id\", \"ground truth\", \"prediction\"])\n",
    "\n",
    "for i, batch_size in enumerate(batch_sizes):\n",
    "    # image_ids\n",
    "    image_ids = [str(x[\"labels\"][\"image_id\"].item()).zfill(5) for x in dataset[\"test\"]][\n",
    "        test_batch_size * i : test_batch_size * i + batch_size\n",
    "    ]\n",
    "\n",
    "    # prediction\n",
    "    image_paths = [\n",
    "        image_dir + x\n",
    "        for x in image_filenames\n",
    "        if x.split(\"_\")[-1].split(\".\")[0] in image_ids\n",
    "    ]\n",
    "    images = [Image.open(x) for x in image_paths]\n",
    "    image_sizes = [[x.size[1], x.size[0]] for x in images]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs.to(\"cuda\"))\n",
    "        target_sizes = torch.tensor(image_sizes)\n",
    "        results = image_processor.post_process_object_detection(\n",
    "            outputs, threshold=0.5, target_sizes=target_sizes\n",
    "        )\n",
    "\n",
    "    for image, result in zip(images, results):\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        for label, box in zip(result[\"labels\"], result[\"boxes\"]):\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            x, y, x2, y2 = tuple(box)\n",
    "            if label == 0:\n",
    "                draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "                draw.text((x, y), id2label[label.item()], fill=\"red\")\n",
    "            elif label == 1:\n",
    "                draw.rectangle((x, y, x2, y2), outline=\"blue\", width=1)\n",
    "                draw.text((x, y), id2label[label.item()], fill=\"blue\")\n",
    "            elif label == 2:\n",
    "                draw.rectangle((x, y, x2, y2), outline=\"green\", width=1)\n",
    "                draw.text((x, y), id2label[label.item()], fill=\"green\")\n",
    "\n",
    "    # ground truth\n",
    "    dataset_org = load_from_disk(dataset_path)\n",
    "    images_objects_org = [\n",
    "        x for x in dataset_org if x[\"image_id\"] in [int(x) for x in image_ids]\n",
    "    ]\n",
    "    images_org = [x[\"image\"].copy() for x in images_objects_org]\n",
    "    objects_org = [x[\"objects\"].copy() for x in images_objects_org]\n",
    "\n",
    "    for image, object_org in zip(images_org, objects_org):\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        for label, box in zip(object_org[\"category\"], object_org[\"bbox\"]):\n",
    "            x, y, w, h = tuple(box)\n",
    "            x2 = x + w\n",
    "            y2 = y + h\n",
    "            if label == 0:\n",
    "                draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "                draw.text((x, y), id2label[label], fill=\"red\")\n",
    "            elif label == 1:\n",
    "                draw.rectangle((x, y, x2, y2), outline=\"blue\", width=1)\n",
    "                draw.text((x, y), id2label[label], fill=\"blue\")\n",
    "            elif label == 2:\n",
    "                draw.rectangle((x, y, x2, y2), outline=\"green\", width=1)\n",
    "                draw.text((x, y), id2label[label], fill=\"green\")\n",
    "\n",
    "    # add datas\n",
    "    for image_id, groundtruth, prediction in zip(image_ids, images_org, images):\n",
    "        table.add_data(image_id, wandb.Image(groundtruth), wandb.Image(prediction))\n",
    "\n",
    "# log table\n",
    "wandb.log({\"Predictions\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c97785-c38e-4dbd-a12b-e3edb711a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bfd99a-e750-4048-8a10-00eff870bdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
